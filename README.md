# missing_spaces

 **Задача** — разработать модель или алгоритм, который принимает на вход текст без пробелов и возвращает восстановленный текст с правильными пробелами и позициями, где они были пропущены. Для решения дообучим модель с Hugging Face, а именно *bert-base-multilingual-cased*

Почему выбран именно такой подход?

Чтобы добиться высокого качества, модель должна быть предобучена и иметь какое-то представление о словах, поэтому был выбран вариант дообучить модель с HF. Обучать в данном случае нужно BERT-подобную модель, потому что модель, например, T5 будет генерировать новую последовательность, что может сильно исказить итоговый вариант строки, нам нужно ли классифицировать, в каких местах нужен пробел.

Обучение производилось на T4 GPU в Google Colab
